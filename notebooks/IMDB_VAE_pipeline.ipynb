{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_qADDiiIgCql"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\"\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "i3wUjdjLg0fW",
    "outputId": "b57778fd-bdb3-4ebf-bc9d-a5f33987f7e1"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = \"twitter.27B.50d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \".\".join(pretrained.split('.')[:2])\n",
    "dim = int(pretrained.split('.')[-1].replace('d',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "from datasets import imdb as dataset\n",
    "import TextCNN.models.textcnn as textcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['text.latex.preamble']=[r\"\\usepackage{lmodern}\"]\n",
    "matplotlib.rcParams['text.latex.preamble'] = [r'\\boldmath']\n",
    "#Options\n",
    "params = {'text.usetex' : False,\n",
    "          'font.size' : 21,\n",
    "         }\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'rt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'imdb':\n",
    "    dataset_root = '/ksozykinraid/data/nlp/IMDB/'\n",
    "    dataset.split_train_valid(path_data='{}/imdb.csv'.format(dataset_root))\n",
    "    pth = torch.load('{}/imdb_text_cnn_best_twitter27b50d.pth'.format(dataset_root),map_location='cpu')\n",
    "elif dataset_name == 'rt':\n",
    "    dataset_root = './'\n",
    "    dataset.split_train_valid(path_data='{}/rt-polarity.csv'.format(dataset_root))\n",
    "    pth = torch.load('{}/rt-polarity_twitter.27B.50d_text_cnn_best.pth'.format(dataset_root),map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_length=32\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.IMDB_Dataset(pretrained=pretrained,fix_length=fix_length,mbsize=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = dataset.TEXT.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = textcnn.textCNN(V, 100, [3,4,5], 0.5 , 2)\n",
    "target.load_state_dict(pth['state_dict'])\n",
    "target.embed = feedEmbed(target.embed )\n",
    "device='cuda'\n",
    "target = target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "disc = textcnn.textCNN(V, 100, [3,4,5], 0.5 , 2)\n",
    "disc.embed = target.embed \n",
    "disc = disc.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_all_grads(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('pth/'):\n",
    "    os.makedirs('pth/')\n",
    "if not os.path.exists('pth_re_train/'):\n",
    "    os.makedirs('pth_re_train/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = dataset.LABEL.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dim = 32\n",
    "lr_decay_every = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.n_train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_VAE(\n",
    "    dataset.n_vocab, h_dim,h_dim,2, p_word_dropout=0.3,\n",
    "    pretrained_embeddings=dataset.get_vocab_vectors(), freeze_embeddings=False,max_sent_len=fix_length,\n",
    "    gpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_itos(x):\n",
    "    return [\" \".join([V.itos[e] for e in s]) for s in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(p):\n",
    "    # [seq_len,probs]\n",
    "    # per 1 elem in batch\n",
    "    # as in paper\n",
    "    with torch.no_grad():\n",
    "        #ppl = -torch.log(F.softmax(p,-1)).sum(1).sum(0)/len(V)\n",
    "        ppl = -F.log_softmax(p,-1).sum(1).sum(0)/len(V)\n",
    "        return ppl.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(re_train,model,disc,phi):\n",
    "    pth = {}\n",
    "    pth['state_dict'] = model.state_dict()\n",
    "    pth['disc_state_dict'] = disc.state_dict()\n",
    "    root = './pth' if not re_train else './pth_re_train'\n",
    "    torch.save(pth, '{}/vae_phi{}.pth'.format(root,phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_attack_rate(N):\n",
    "    j = 0\n",
    "    rate = []\n",
    "    perp = []\n",
    "    for _ in range(N):\n",
    "        z = model.sample_z_prior(1)\n",
    "        c = model.sample_c_prior(1)\n",
    "        z=torch.cat([z,model.C[torch.argmax(c)].unsqueeze(0)],-1)\n",
    "\n",
    "\n",
    "        sample_idxs,sample_logits = model.sample_sentence(z, c,temp=0.8)\n",
    "        sample_sent = dataset.idxs2sentence(sample_idxs)\n",
    "        ppl = perplexity(sample_logits)\n",
    "        sample_logits = sample_logits.unsqueeze(0)\n",
    "\n",
    "\n",
    "        if sample_logits.shape[1] < 5:\n",
    "            continue\n",
    "        j += 1\n",
    "        if j >= N:\n",
    "            break\n",
    "        c_idx = torch.argmax(c,-1).cpu().numpy()[0]\n",
    "        pred_label_idx =  torch.argmax(target(sample_logits),-1)[0].cpu().numpy()\n",
    "        rate.append(c_idx !=  pred_label_idx)\n",
    "        perp.append(ppl)\n",
    "    perp = np.mean(ppl)\n",
    "    attack_rate=np.mean(rate)\n",
    "    return attack_rate,perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_disriminator = True\n",
    "from_scratch = False # do we used checkpoints or not\n",
    "from_scratch_re_train = False\n",
    "generate_and_save = False # used for re-training on adversarial data\n",
    "tune_on_z  = False # used for re-training on adversarial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1 = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(phi = 0,\n",
    "          num_epochs=5,\n",
    "          log_interval = 10,\n",
    "          re_train=False,\n",
    "          lr = 1e-2,\n",
    "          gen_batch_size=8):\n",
    "    vanila = (phi == 0)\n",
    "    print(\"phi={}\".format(phi))\n",
    "    n_iter = num_epochs*dataset.n_train_batch\n",
    "    # Annealing for KL term\n",
    "    kld_start_inc = 1000\n",
    "    kld_weight = 0.0\n",
    "    kld_max = 1\n",
    "    if not vanila:\n",
    "        kld_weight =  kld_max\n",
    "    kld_inc = (kld_max - kld_weight) / (n_iter - kld_start_inc)\n",
    "    it = 0\n",
    "    loss_data = []\n",
    "    recon_loss_data = []\n",
    "    kl_loss_data = []\n",
    "    opt = optim.Adam(model.vae_params + list(target.parameters()), lr=lr,betas=(beta1, 0.999))\n",
    "    opt_disc = optim.Adam(disc.parameters(), lr=lr,betas=(beta1, 0.999))\n",
    "    for epoch in range(num_epochs):\n",
    "      #print(\"epoch\",epoch)\n",
    "      for idx,batch in enumerate(dataset.train_iter):\n",
    "            x, labels = batch.text, batch.label\n",
    "            c  = torch.eye(2)[labels]\n",
    "            x, labels = x.to(device), labels.to(device)\n",
    "            c = c.to(device)\n",
    "\n",
    "            recon_loss, kl_loss, x_hat = model.forward(x,c)\n",
    "            loss = recon_loss + kld_weight * kl_loss\n",
    "\n",
    "\n",
    "            #Anneal kl_weight\n",
    "            if it > kld_start_inc and kld_weight < kld_max:\n",
    "                kld_weight += kld_inc\n",
    "\n",
    "            loss = loss.reshape(1)\n",
    "            attack_rate = 0\n",
    "\n",
    "            if not vanila:\n",
    "                x_adv, labels_adv = model.generate_sentences(gen_batch_size)\n",
    "                pred_adv = target(x_adv)\n",
    "                Ladv = F.cross_entropy(pred_adv,torch.argmax(labels_adv.long(),-1))\n",
    "\n",
    "                labels_adv_idx =  torch.argmax(labels_adv.long(),-1).cpu().numpy()\n",
    "                pred_label_idx =  torch.argmax(pred_adv,-1).cpu().numpy()\n",
    "\n",
    "                attack_rate=np.mean((labels_adv_idx !=  pred_label_idx))\n",
    "                \n",
    "                loss -= phi*Ladv\n",
    "                \n",
    "                if with_disriminator:\n",
    "                    x_real = torch.eye(len(V))[x].to(device)\n",
    "                    x_real = torch.transpose(x_real,0, 1)\n",
    "                    pred_real = disc(x_real)\n",
    "                    Ldisc_real = F.cross_entropy(pred_real,labels)\n",
    "                    Ldisc_real.backward()\n",
    "                    #https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "                    pred_fake = disc(x_adv.detach())\n",
    "                    Ldisc_fake = F.cross_entropy(pred_fake,torch.argmax(labels_adv.long(),-1))\n",
    "                    Ldisc_fake.backward()\n",
    "\n",
    "                    Ldisc = Ldisc_real + Ldisc_fake\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm(disc.parameters(), 1)\n",
    "                    opt_disc.step()\n",
    "                    opt_disc.zero_grad()\n",
    "                    loss += loss + Ldisc.detach()\n",
    "\n",
    "                \n",
    "                #print('loss',loss.data)\n",
    "\n",
    "            loss.backward()\n",
    "            #grad_norm = torch.nn.utils.clip_grad_norm(model.vae_params, 1)\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            loss_data.append(loss.data)\n",
    "            recon_loss_data.append(recon_loss.reshape(1).data)\n",
    "            kl_loss_data.append(kl_loss.reshape(1).data)\n",
    "\n",
    "\n",
    "            if it % log_interval == 0:\n",
    "\n",
    "                save_model(re_train,model,disc,phi)\n",
    "                x_text = batch_itos(x)\n",
    "                x_text_hat = batch_itos(torch.argmax(x_hat,-1))\n",
    "\n",
    "                z = model.sample_z_prior(1)\n",
    "                c = model.sample_c_prior(1)\n",
    "                z=torch.cat([z,model.C[torch.argmax(c)].unsqueeze(0)],-1)\n",
    "\n",
    "                sample_idxs,sample_logits = model.sample_sentence(z, c)\n",
    "                sample_sent = dataset.idxs2sentence(sample_idxs)\n",
    "\n",
    "                sample_logits = sample_logits.unsqueeze(0)\n",
    "\n",
    "                if sample_logits.shape[1] < 5:\n",
    "                    continue\n",
    "\n",
    "                pred_label_idx =  torch.argmax(target(sample_logits),-1)[0]\n",
    "\n",
    "                c_idx = torch.argmax(c)\n",
    "                print('epoch-%d Iter-%d; Loss: %9.4f; Recon: %9.4f; KL: %9.4f;'%(epoch,it, loss.data, \n",
    "                                                                                 recon_loss.data, kl_loss.data))\n",
    "                print(sample_sent)\n",
    "                print('sampled',itos[c_idx],'predicted',itos[pred_label_idx])\n",
    "                print('attack_rate per batch',attack_rate)\n",
    "                print('perplexity per sampled sentance',perplexity(sample_logits[0]))\n",
    "                print()\n",
    "            # Anneal learning rate\n",
    "            #new_lr = lr * (0.5 ** (it // lr_decay_every))\n",
    "            #for param_group in opt.param_groups:\n",
    "            #    param_group['lr'] = new_lr\n",
    "            it += 1\n",
    "    \n",
    "    return loss_data,recon_loss_data, kl_loss_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval =  1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if from_scratch:\n",
    "    loss_data0,recon_loss_data0, kl_loss_data0 = train(phi=0,num_epochs=50,log_interval = log_interval)\n",
    "    a,ppl = test_attack_rate(500)\n",
    "    print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if from_scratch:\n",
    "    LD = []\n",
    "    RLD = []\n",
    "    KLD = []\n",
    "    for i in range(0, len(loss_data0)):\n",
    "        LD.append(loss_data0[i].data.cpu().numpy()[0])\n",
    "        RLD.append(recon_loss_data0[i].data.cpu().numpy()[0])\n",
    "        KLD.append(kl_loss_data0[i].data.cpu().numpy()[0])\n",
    "\n",
    "\n",
    "    plt.figure(facecolor='white')\n",
    "    plt.plot(LD[::10], marker='o',label='LD')\n",
    "    plt.plot(RLD[::10], marker='o',label='RLD')\n",
    "    plt.plot(KLD[::10], marker='o',label='KLD')\n",
    "    plt.grid()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phis=[0,1,3,6,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if from_scratch:\n",
    "    \n",
    "    for phi in phis[1:]:\n",
    "        vae_phi0_pth = torch.load('./pth/vae_phi0.pth',map_location='cpu')\n",
    "        try:\n",
    "            model.load_state_dict(vae_phi0_pth['state_dict'])\n",
    "            \n",
    "            disc = textcnn.textCNN(V, 100, [3,4,5], 0.5 , 2)\n",
    "            disc.embed = target.embed \n",
    "            disc = disc.to(device)\n",
    "            model.eval()\n",
    "\n",
    "            disable_all_grads_enoder(model)\n",
    "        except:\n",
    "            pass\n",
    "        log_interval = 100\n",
    "        num_epochs = 3\n",
    "        loss_data,recon_loss_data, kl_loss_data = train(phi=phi,num_epochs=num_epochs,lr = 1e-2,\n",
    "                                                        log_interval = log_interval)\n",
    "        a,ppl = test_attack_rate(500)\n",
    "        print('test attack rate',a)\n",
    "        print('test ppl',ppl)\n",
    "        #attack_rates_test.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(root='./pth',phis=[0,1,3,6,9],verbose=False,n_samples=500):\n",
    "    at = []\n",
    "    perp = []\n",
    "    for phi in phis:\n",
    "        if verbose:\n",
    "            print(phi)\n",
    "        vae_phi0_pth = torch.load('{}/vae_phi{}.pth'.format(root,phi),map_location='cpu')\n",
    "        try:\n",
    "            model.load_state_dict(vae_phi0_pth['state_dict'])\n",
    "            model.eval()\n",
    "        except:\n",
    "            pass\n",
    "        a,ppl = test_attack_rate(n_samples)\n",
    "        if verbose:\n",
    "            print('test attack rate',a)\n",
    "            print('test ppl',ppl)\n",
    "        perp.append(ppl)\n",
    "        at.append(a)\n",
    "    model.train()\n",
    "    return phis,at,perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_trials = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phis0,at0,perp0 = [],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(N_trials)):\n",
    "    phis_,at_,perp_ = get_metrics(phis=phis)\n",
    "    phis0.append(phis_)\n",
    "    at0.append(at_)\n",
    "    perp0.append(perp_)\n",
    "phis0 = np.array(phis0).mean(0)\n",
    "at0 = np.array(at0).mean(0)\n",
    "perp0 = np.array(perp0).mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_plots(phis_,at_,perp_):\n",
    "    print('phis:',phis_)\n",
    "    print('attack_rates:',at_)\n",
    "    print('ppl:',perp_)\n",
    "    plt.figure(facecolor='white',figsize=(20,20))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(phis_,at_,marker='o',color='red',lw=4,markersize=12)\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"phi\")\n",
    "    plt.ylabel(\"attack rate\")\n",
    "    plt.ylim(0)\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(phis_,perp_,marker='o',color='g',lw=4,markersize=12)\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"phi\")\n",
    "    plt.ylabel(\"perplexity\")\n",
    "    plt.ylim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_plots(phis0,at0,perp0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples for validity rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for phi in phis:\n",
    "    vae_phi_pth = torch.load('./pth/vae_phi{}.pth'.format(phi),map_location='cpu')\n",
    "    try:\n",
    "        model.load_state_dict(vae_phi_pth['state_dict'])\n",
    "        disable_all_grads_enoder(model)\n",
    "    except:\n",
    "        pass\n",
    "    for i in range(batch_size):\n",
    "        z = model.sample_z_prior(1)\n",
    "        c = model.sample_c_prior(1)\n",
    "        z=torch.cat([z,model.C[torch.argmax(c)].unsqueeze(0)],-1)\n",
    "\n",
    "        sample_idxs,sample_logits = model.sample_sentence(z, c,temp=0.8)\n",
    "        sample_sent = dataset.idxs2sentence(sample_idxs)\n",
    "        lb = torch.argmax(c)\n",
    "        sample_label = itos[lb]\n",
    "        samples.append([phi,sample_sent,sample_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.DataFrame(samples)\n",
    "samples.columns = ['phi','sample_sent','sample_label',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.to_csv('validity_rate_sample.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_adv = 6000//batch_size\n",
    "if generate_and_save:\n",
    "    vae_phi9_pth = torch.load('./pth/vae_phi{}.pth'.format(phis[-1]),map_location='cpu')\n",
    "    try:\n",
    "        model.load_state_dict(vae_phi0_pth['state_dict'])\n",
    "            \n",
    "        disc = textcnn.textCNN(V, 100, [3,4,5], 0.5 , 2)\n",
    "        disc.embed = target.embed \n",
    "        disc = disc.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        disable_all_grads_enoder(model)\n",
    "    except:\n",
    "        pass\n",
    "    X_adv = []\n",
    "    L_adv = []\n",
    "    for i in tqdm(range(N_adv)):\n",
    "        with torch.no_grad():\n",
    "            x_adv, labels_adv = model.generate_sentences(batch_size)\n",
    "            x_adv, labels_adv = x_adv.cpu(), labels_adv.cpu()\n",
    "            X_adv.append(x_adv)\n",
    "            L_adv.append(labels_adv)\n",
    "    adv_data = {}\n",
    "    adv_data['x'] = X_adv\n",
    "    adv_data['l'] = L_adv \n",
    "    torch.save(adv_data, './adv_data.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if tune_on_z:\n",
    "    print('re-training')\n",
    "    target = textcnn.textCNN(V, 100, [3,4,5], 0.5 , 2)\n",
    "    target.load_state_dict(pth['state_dict'])\n",
    "    target.embed = feedEmbed(target.embed )\n",
    "    target = target.to(device)\n",
    "    enable_all_grads(target)\n",
    "    optimizer = optim.Adam(target.parameters(), lr=1e-3)\n",
    "    total_loss = []\n",
    "    total_attack = []\n",
    "    it = 0\n",
    "    for epoch in range(9):\n",
    "        num_uncorrect = 0\n",
    "        num_samples = 0\n",
    "        for x_adv,l_adv in zip(adv_data['x'],adv_data['l']):\n",
    "            x_adv,l_adv = x_adv.cuda(),l_adv.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            pred_adv = target(x_adv)\n",
    "\n",
    "            Ladv = F.cross_entropy(pred_adv,torch.argmax(l_adv.long(),-1))\n",
    "            Ladv.backward()\n",
    "            total_loss.append(Ladv.data.cpu().numpy())\n",
    "            if it % 100 == 0:\n",
    "                print(total_loss[-1])\n",
    "            optimizer.step()\n",
    "            it += 1\n",
    "            c_idx = torch.argmax(l_adv,-1).cpu().numpy()\n",
    "            pred =  torch.argmax(pred_adv,-1).cpu().numpy()\n",
    "            num_uncorrect += (pred != c_idx).sum()\n",
    "            num_samples += len(c_idx) \n",
    "        a = num_uncorrect/num_samples\n",
    "        total_attack.append(a)\n",
    "        print('epoch',epoch,a)\n",
    "    disable_all_grads(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if from_scratch_re_train:\n",
    "    for phi in phis[1:]:\n",
    "        vae_phi0_pth = torch.load('./pth/vae_phi{}.pth'.format(phi),map_location='cpu')\n",
    "        try:\n",
    "            model.load_state_dict(vae_phi0_pth['state_dict'])\n",
    "            disc = textcnn.textCNN(V, 100, [3,4,5], 0.5 , 2)\n",
    "            disc.embed = target.embed \n",
    "            disc = disc.to(device)\n",
    "            model.eval()\n",
    "\n",
    "            disable_all_grads_enoder(model)\n",
    "        except:\n",
    "            pass\n",
    "        log_interval = 100\n",
    "        num_epochs = 2\n",
    "        loss_data,recon_loss_data, kl_loss_data = train(phi=phi,num_epochs=num_epochs,lr = 1e-2,\n",
    "                                                        log_interval = log_interval,re_train=True)\n",
    "        a,ppl = test_attack_rate(100)\n",
    "        print('test attack rate',a)\n",
    "        print('test ppl',ppl)\n",
    "        #attack_rates_test.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./pth/vae_phi0.pth ./pth_re_train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phis1,at1,perp1 = [],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(N_trials)):\n",
    "    phis_,at_,perp_ = get_metrics(root='./pth_re_train',phis=phis)\n",
    "    phis1.append(phis_)\n",
    "    at1.append(at_)\n",
    "    perp1.append(perp_)\n",
    "phis1 = np.array(phis1).mean(0)\n",
    "at1 = np.array(at1).mean(0)\n",
    "perp1 = np.array(perp1).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_plots(phis1,at1,perp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comp_plots(phis0,at0,perp0,phis1,at1,perp1):\n",
    "    print('phis:',phis0)\n",
    "    print('attack_rates:',at0)\n",
    "    print('ppl:',perp0)\n",
    "    print('phis:',phis1)\n",
    "    print('attack_rates:',at1)\n",
    "    print('ppl:',perp1)\n",
    "    plt.figure(facecolor='white',figsize=(20,20))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(phis0,at0,marker='o',color='red',label='before re-training',lw=4,markersize=12)\n",
    "    plt.plot(phis1,at1,marker='o',color='blue',label='after re-training',lw=4,markersize=12)\n",
    "    plt.grid()\n",
    "    plt.legend(loc='best',fancybox=True,shadow=True,framealpha=0.99)\n",
    "    plt.xlabel(\"phi\")\n",
    "    plt.ylabel(\"attack rate\")\n",
    "    plt.ylim(0)\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(phis0,perp0,marker='o',color='green',label='before re-training',lw=4,markersize=12)\n",
    "    plt.plot(phis1,perp1,marker='o',color='magenta',label='after re-training',lw=4,markersize=12)\n",
    "    plt.grid()\n",
    "    plt.legend(loc='best',fancybox=True,shadow=True,framealpha=0.99)\n",
    "    plt.xlabel(\"phi\")\n",
    "    plt.ylabel(\"perplexity\")\n",
    "    plt.ylim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comp_plots(phis0,at0,perp0,phis1,at1,perp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('phis:',phis0)\n",
    "print('attack_rates:',at0)\n",
    "print('ppl:',perp0)\n",
    "print('phis:',phis1)\n",
    "print('attack_rates:',at1)\n",
    "print('ppl:',perp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_data = {\n",
    "    \n",
    "    \"phis\": phis,\n",
    "    \"attack_rates0\": at0,\n",
    "    \"ppl0\": perp0,\n",
    "    \"attack_rates1\": at1,\n",
    "    \"ppl1\": perp1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(plots_data,\"plots_data.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IMDB_VAE_pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
